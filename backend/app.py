from flask import Flask, jsonify
from flask_cors import CORS
from scraper import NoticeScraper
import re
import threading
import time
from datetime import datetime
import atexit
import json
import os

app = Flask(__name__)
CORS(app)

# ===== ì„¤ì • =====
CACHE_FILE_PATH = os.path.join(os.path.dirname(__file__), 'cache.json')
CACHE_UPDATE_INTERVAL = 3000  # 50ë¶„ë§ˆë‹¤ ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ë§
MAIN_PAGE_START = 2  # 2í˜ì´ì§€ë¶€í„° (1í˜ì´ì§€ëŠ” ìƒë‹¨ê³µì§€)
MAIN_PAGE_END = 5

# ===== ì†ŒìŠ¤ ì •ì˜ =====
SOURCES = {
    "library": {
        "name": "ë„ì„œê´€",
        "color": "#43a047",
        "icon": "ğŸ“š"
    },
    "main": {
        "name": "ë©”ì¸ê³µì§€",
        "color": "#1a73e8",
        "icon": "ğŸ«"
    },
    "fusion": {
        "name": "ìœµí•©êµìœ¡",
        "color": "#9c27b0",
        "icon": "ğŸ”¬"
    },
    "academic": {
        "name": "í•™ì‚¬",
        "color": "#f44336",
        "icon": "ğŸ“"
    },
    "scholarship": {
        "name": "ì¥í•™",
        "color": "#ff9800",
        "icon": "ğŸ’°"
    },
    "volunteer": {
        "name": "ì‚¬íšŒë´‰ì‚¬",
        "color": "#4caf50",
        "icon": "ğŸ¤"
    },
    "external": {
        "name": "ì™¸ë¶€ê³µì§€",
        "color": "#607d8b",
        "icon": "ğŸ“¢"
    },
    "career": {
        "name": "ì·¨ì—…",
        "color": "#2196f3",
        "icon": "ğŸ’¼"
    },
    "cando": {
        "name": "ìº”ë‘",
        "color": "#e91e63",
        "icon": "ğŸ¯"
    }
}

# ===== ìºì‹œ ë° ìƒíƒœ ê´€ë¦¬ =====
cache = {source: {"data": [], "tags": [], "last_updated": None} for source in SOURCES}

cache_lock = threading.Lock()
scraper_lock = threading.Lock()
scraper = None
background_thread = None
is_running = True

def load_cache_from_file():
    """JSON íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
    global cache
    try:
        if os.path.exists(CACHE_FILE_PATH):
            with open(CACHE_FILE_PATH, 'r', encoding='utf-8') as f:
                loaded_cache = json.load(f)
                with cache_lock:
                    for source_key in SOURCES:
                        if source_key in loaded_cache:
                            cache[source_key] = loaded_cache[source_key]
                print(f"[{datetime.now()}] ìºì‹œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {CACHE_FILE_PATH}")
                return True
    except Exception as e:
        print(f"[WARNING] ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return False

def save_cache_to_file():
    """ìºì‹œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        with cache_lock:
            cache_copy = {k: v.copy() for k, v in cache.items()}
        
        with open(CACHE_FILE_PATH, 'w', encoding='utf-8') as f:
            json.dump(cache_copy, f, ensure_ascii=False, indent=2)
        print(f"[{datetime.now()}] ìºì‹œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {CACHE_FILE_PATH}")
        return True
    except Exception as e:
        print(f"[ERROR] ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def get_scraper():
    global scraper
    with scraper_lock:
        if scraper is None:
            scraper = NoticeScraper()
        return scraper

def reset_scraper():
    """ë“œë¼ì´ë²„ ì„¸ì…˜ ë¬¸ì œ ì‹œ ìŠ¤í¬ë˜í¼ ì¬ìƒì„±"""
    global scraper
    with scraper_lock:
        if scraper is not None:
            try:
                scraper.close()
            except:
                pass
            scraper = None
        scraper = NoticeScraper()
        return scraper

def close_scraper():
    global scraper
    with scraper_lock:
        if scraper is not None:
            try:
                scraper.close()
            except:
                pass
            scraper = None

def extract_tags(title):
    """ì œëª©ì—ì„œ [xxxx] í˜•íƒœì˜ íƒœê·¸ ì¶”ì¶œ"""
    tags = re.findall(r'\[([^\]]+)\]', title)
    return tags

def process_notices(notices, source):
    """ê³µì§€ì‚¬í•­ ë°ì´í„°ì— íƒœê·¸ ì •ë³´ ì¶”ê°€"""
    processed = []
    for i in range(len(notices["ì œëª©"])):
        title = notices["ì œëª©"][i]
        link = notices["ë§í¬"][i]
        tags = extract_tags(title)
        
        notice_data = {
            "id": f"{source}-{i + 1}",
            "title": title,
            "link": link,
            "date": notices["ë‚ ì§œ"][i],
            "tags": tags,
            "source": source,
            "sourceName": SOURCES[source]["name"],
            "sourceColor": SOURCES[source]["color"],
            "sourceIcon": SOURCES[source]["icon"]
        }
        
        # ìº”ë‘ì˜ ê²½ìš° ìƒíƒœ ì •ë³´ ì¶”ê°€
        if "ìƒíƒœ" in notices and i < len(notices["ìƒíƒœ"]):
            notice_data["status"] = notices["ìƒíƒœ"][i]
        
        processed.append(notice_data)
    return processed

def merge_notices(existing_data, new_data, source_key=None):
    """ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„°ë¥¼ ë³‘í•© (ì¤‘ë³µ ì œê±°, ìƒˆ ê³µì§€ ì¶”ê°€)"""
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ (title, link) ê¸°ì¤€ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬í™”
    existing_map = {}
    for notice in existing_data:
        key = (notice.get("title", ""), notice.get("link", ""))
        existing_map[key] = notice
    
    # ìƒˆ ë°ì´í„° ë³‘í•© (ìƒˆ ê³µì§€ ì¶”ê°€, ê¸°ì¡´ ê³µì§€ ì—…ë°ì´íŠ¸)
    new_count = 0
    updated_count = 0
    status_changed_count = 0
    
    for notice in new_data:
        key = (notice.get("title", ""), notice.get("link", ""))
        if key not in existing_map:
            # ìƒˆë¡œìš´ ê³µì§€ ì¶”ê°€
            existing_map[key] = notice
            new_count += 1
        else:
            # ìº”ë‘ì˜ ê²½ìš° ìƒíƒœ ë³€í™” ê°ì§€
            if source_key == "cando":
                old_status = existing_map[key].get("status", "")
                new_status = notice.get("status", "")
                if old_status != new_status:
                    status_changed_count += 1
                    print(f"[STATUS] '{notice.get('title', '')[:30]}...' ìƒíƒœ ë³€ê²½: {old_status} â†’ {new_status}")
            
            # ê¸°ì¡´ ê³µì§€ ì—…ë°ì´íŠ¸
            existing_map[key].update(notice)
            updated_count += 1
    
    # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    merged_list = list(existing_map.values())
    
    return merged_list, new_count, updated_count, status_changed_count

def crawl_source(source_key):
    """íŠ¹ì • ì†ŒìŠ¤ í¬ë¡¤ë§"""
    max_retries = 2
    
    for attempt in range(max_retries):
        try:
            s = get_scraper()
            
            if source_key == "library":
                notices = s.library()
            elif source_key == "main":
                notices = s.main_pg(page_start=MAIN_PAGE_START, page_end=MAIN_PAGE_END)
            elif source_key == "fusion":
                notices = s.main_fusion()
            elif source_key == "academic":
                notices = s.main_academic()
            elif source_key == "scholarship":
                notices = s.main_scholarship()
            elif source_key == "volunteer":
                notices = s.main_volunteer()
            elif source_key == "external":
                notices = s.main_external()
            elif source_key == "career":
                notices = s.main_career()
            elif source_key == "cando":
                notices = s.cando()
            else:
                return None, None
            
            processed = process_notices(notices, source_key)
            
            all_tags = set()
            for notice in processed:
                all_tags.update(notice["tags"])
            
            return processed, list(all_tags)
        except Exception as e:
            error_msg = str(e).lower()
            print(f"[ERROR] {source_key} í¬ë¡¤ë§ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            
            # ì„¸ì…˜ ê´€ë ¨ ì˜¤ë¥˜ë©´ ë“œë¼ì´ë²„ ì¬ìƒì„±
            if "invalid session" in error_msg or "session" in error_msg or "disconnected" in error_msg:
                print(f"[INFO] ì„¸ì…˜ ì˜¤ë¥˜ ê°ì§€, ë“œë¼ì´ë²„ ì¬ìƒì„±...")
                reset_scraper()
            
            if attempt == max_retries - 1:
                return None, None
    
    return None, None

def update_cache():
    """ì „ì²´ ìºì‹œ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€, ìƒˆ ë°ì´í„° ë³‘í•©)"""
    global cache
    
    print(f"[{datetime.now()}] ìºì‹œ ì—…ë°ì´íŠ¸ ì‹œì‘...")
    
    consecutive_failures = 0
    max_failures = 3
    updated_count = 0
    total_new = 0
    total_status_changed = 0
    
    for source_key in SOURCES:
        try:
            data, tags = crawl_source(source_key)
            if data is not None:
                with cache_lock:
                    # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
                    existing_data = cache[source_key]["data"]
                    merged_data, new_count, upd_count, status_changed = merge_notices(existing_data, data, source_key)
                    
                    # ID ì¬í• ë‹¹ (ë³‘í•© í›„ ìˆœì„œ ì •ë¦¬)
                    for i, notice in enumerate(merged_data):
                        notice["id"] = f"{source_key}-{i + 1}"
                    
                    # íƒœê·¸ ë³‘í•©
                    existing_tags = set(cache[source_key]["tags"])
                    existing_tags.update(tags)
                    
                    cache[source_key]["data"] = merged_data
                    cache[source_key]["tags"] = list(existing_tags)
                    cache[source_key]["last_updated"] = datetime.now().isoformat()
                
                # ë¡œê·¸ ì¶œë ¥
                log_msg = f"[{datetime.now()}] {SOURCES[source_key]['name']} ìºì‹œ ì—…ë°ì´íŠ¸: ì´ {len(merged_data)}ê±´ (ì‹ ê·œ {new_count}ê±´"
                if source_key == "cando" and status_changed > 0:
                    log_msg += f", ìƒíƒœë³€ê²½ {status_changed}ê±´"
                log_msg += ")"
                print(log_msg)
                
                consecutive_failures = 0
                updated_count += 1
                total_new += new_count
                total_status_changed += status_changed
            else:
                consecutive_failures += 1
        except Exception as e:
            print(f"[ERROR] {source_key} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            consecutive_failures += 1
        
        # ì—°ì† ì‹¤íŒ¨ ì‹œ ë“œë¼ì´ë²„ ì¬ìƒì„±
        if consecutive_failures >= max_failures:
            print(f"[WARNING] ì—°ì† {max_failures}íšŒ ì‹¤íŒ¨, ë“œë¼ì´ë²„ ì¬ìƒì„± ì¤‘...")
            reset_scraper()
            consecutive_failures = 0
    
    # ìºì‹œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    if updated_count > 0:
        save_cache_to_file()
    
    # ìµœì¢… ë¡œê·¸
    final_log = f"[{datetime.now()}] ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ! ({updated_count}/{len(SOURCES)} ì†ŒìŠ¤, ì‹ ê·œ {total_new}ê±´"
    if total_status_changed > 0:
        final_log += f", ìƒíƒœë³€ê²½ {total_status_changed}ê±´"
    final_log += ")"
    print(final_log)

def background_crawler():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ í¬ë¡¤ë§"""
    global is_running
    
    time.sleep(2)
    update_cache()
    
    while is_running:
        time.sleep(CACHE_UPDATE_INTERVAL)
        if is_running:
            update_cache()

def start_background_crawler():
    """ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ëŸ¬ ì‹œì‘"""
    global background_thread
    
    # ë¨¼ì € ìºì‹œ íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
    cache_loaded = load_cache_from_file()
    
    background_thread = threading.Thread(target=background_crawler, daemon=True)
    background_thread.start()
    
    if cache_loaded:
        print(f"[{datetime.now()}] ê¸°ì¡´ ìºì‹œ ë¡œë“œë¨, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì—…ë°ì´íŠ¸ ì§„í–‰")
    print(f"[{datetime.now()}] ë°±ê·¸ë¼ìš´ë“œ í¬ë¡¤ëŸ¬ ì‹œì‘ë¨ (ì—…ë°ì´íŠ¸ ì£¼ê¸°: {CACHE_UPDATE_INTERVAL}ì´ˆ)")

def shutdown_handler():
    """ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    global is_running
    is_running = False
    save_cache_to_file()  # ì¢…ë£Œ ì „ ìºì‹œ ì €ì¥
    close_scraper()
    print("ì„œë²„ ì¢…ë£Œ ì²˜ë¦¬ ì™„ë£Œ")

atexit.register(shutdown_handler)

# ===== API ì—”ë“œí¬ì¸íŠ¸ =====

@app.route('/api/sources', methods=['GET'])
def get_sources():
    """ì†ŒìŠ¤ ëª©ë¡ API"""
    return jsonify({
        "success": True,
        "sources": SOURCES
    })

@app.route('/api/all', methods=['GET'])
def get_all_notices():
    """ì „ì²´ ê³µì§€ì‚¬í•­ í†µí•© API"""
    all_notices = []
    all_tags = set()
    source_counts = {}
    
    with cache_lock:
        for source_key in SOURCES:
            source_data = cache[source_key]["data"]
            source_tags = cache[source_key]["tags"]
            
            all_notices.extend(source_data)
            all_tags.update(source_tags)
            source_counts[source_key] = len(source_data)
    
    # ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
    def parse_date(notice):
        date_str = notice.get("date", "")
        try:
            # YYYY.MM.DD ë˜ëŠ” YYYY-MM-DD í˜•ì‹
            clean_date = date_str.replace(".", "-").replace("/", "-")
            return clean_date
        except:
            return "0000-00-00"
    
    all_notices.sort(key=parse_date, reverse=True)
    
    return jsonify({
        "success": True,
        "notices": all_notices,
        "tags": list(all_tags),
        "sources": SOURCES,
        "sourceCounts": source_counts,
        "cached": True
    })

@app.route('/api/refresh', methods=['POST'])
def force_refresh():
    """ê°•ì œ ìºì‹œ ê°±ì‹  API"""
    try:
        update_cache()
        return jsonify({
            "success": True,
            "message": "ìºì‹œ ê°±ì‹  ì™„ë£Œ"
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/status', methods=['GET'])
def get_status():
    """ì„œë²„ ìƒíƒœ ë° ìºì‹œ ì •ë³´"""
    with cache_lock:
        cache_info = {}
        for source_key in SOURCES:
            cache_info[source_key] = {
                "name": SOURCES[source_key]["name"],
                "count": len(cache[source_key]["data"]),
                "last_updated": cache[source_key]["last_updated"]
            }
        
        # ìºì‹œ íŒŒì¼ ì •ë³´
        cache_file_exists = os.path.exists(CACHE_FILE_PATH)
        cache_file_size = os.path.getsize(CACHE_FILE_PATH) if cache_file_exists else 0
        cache_file_modified = datetime.fromtimestamp(os.path.getmtime(CACHE_FILE_PATH)).isoformat() if cache_file_exists else None
        
        return jsonify({
            "status": "ok",
            "cache": cache_info,
            "cacheFile": {
                "path": CACHE_FILE_PATH,
                "exists": cache_file_exists,
                "size": cache_file_size,
                "lastModified": cache_file_modified
            },
            "settings": {
                "update_interval_seconds": CACHE_UPDATE_INTERVAL
            }
        })

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({"status": "ok"})

if __name__ == '__main__':
    start_background_crawler()
    app.run(debug=False, port=5000, threaded=True)
